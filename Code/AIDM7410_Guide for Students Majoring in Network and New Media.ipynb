{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1031\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re \n",
    "\n",
    "#Replace the code that cannot be displayed with the cracked code\n",
    "#The encrypted information is changing every day, so every crawling needs to be re decrypted\n",
    "def decode_font(data):\n",
    "    data = data.replace(b'\\xef\\x9d\\x9e', b'0')\n",
    "    data = data.replace(b'\\xee\\xb4\\x94', b'1')\n",
    "    data = data.replace(b'\\xee\\xb8\\xb8', b'2')\n",
    "    data = data.replace(b'\\xef\\x9c\\x9c', b'3')\n",
    "    data = data.replace(b'\\xee\\xb6\\xbb', b'4')\n",
    "    data = data.replace(b'\\xee\\xa8\\x86', b'5')\n",
    "    data = data.replace(b'\\xee\\xa1\\xb9', b'6')\n",
    "    data = data.replace(b'\\xee\\xb4\\xab', b'7')\n",
    "    data = data.replace(b'\\xee\\xb4\\xab', b'8')\n",
    "    data = data.replace(b'\\xee\\xa1\\xa9', b'9')\n",
    "    data = data.decode('utf-8')\n",
    "    return data\n",
    "\n",
    "headers={\n",
    "    'Connection':'close',\n",
    "    \"User-Agent\": 'Mozilla/5.0'\n",
    "}\n",
    "sess = requests.Session()\n",
    "detail_urls=[]\n",
    "for j in range(201,255):\n",
    "    \n",
    "    url=f'https://www.shixiseng.com/interns?page={j}&type=intern&keyword=%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%96%B0%E5%AA%92%E4%BD%93&area=&months=&days=&degree=&official=&enterprise=&salary=-0&publishTime=&sortType=&city=%E5%85%A8%E5%9B%BD&internExtend='\n",
    "    page_text = sess.get(url=url,headers=headers).text\n",
    "    soup = BeautifulSoup(page_text,'lxml') \n",
    "    for i in range(len(soup.findAll('div',class_='intern-wrap intern-item'))):\n",
    "        detail_url = soup.select('.f-l.intern-detail__job a')[i]['href']\n",
    "        title = soup.select('.f-r.intern-detail__company>p>a')[i]['title']\n",
    "        #Get links to details\n",
    "#         print(title,detail_url)\n",
    "        detail_urls.append(detail_url)\n",
    "print(len(detail_urls))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Crawling recruitment website data\n",
    "\n",
    "job_names=[]\n",
    "comps=[]\n",
    "salaries=[]\n",
    "locates=[]\n",
    "degrees=[]\n",
    "weekdays=[]\n",
    "deadlines=[]\n",
    "job_details=[]\n",
    "for url in detail_urls:\n",
    "    page_text = sess.get(url=url,headers=headers).text\n",
    "    soupa = BeautifulSoup(page_text,'lxml') \n",
    "    #1.find div class=new_job_name,and find岗位名称\n",
    "    job_name = soup.select('.new_job_name')[0].text.strip()\n",
    "    #2.find公司名称\n",
    "    comp = soup.select('div.com_intro > a.com-name')[0].text.strip()\n",
    "    #3.find工资\n",
    "    salary = decode_font(soup.select('.job_msg>span')[0].text.encode('utf-8'))\n",
    "    #4.find地区\n",
    "    locate = soup.select('.job_msg>span')[1]['title']\n",
    "    #5.find学位要求\n",
    "    degree = soup.select('.job_msg>span')[2].text\n",
    "    #6.find每星期上班天数\n",
    "    weekday = decode_font(soup.select('.job_msg>span')[3].text.encode('utf-8'))\n",
    "    #7.find岗位描述\n",
    "    job_detail = soup.select('.job_detail')[0].text.strip()\n",
    "    bytes(job_detail, encoding = \"utf8\")\n",
    "    #8.find截止日期\n",
    "    deadline = decode_font(soup.select('.cutom_font')[-1].text.encode('utf-8'))\n",
    "    job_names.append(job_name)\n",
    "    comps.append(comp)\n",
    "    salaries.append(salary)\n",
    "    locates.append(locate)\n",
    "    degrees.append(degree)\n",
    "    weekdays.append(weekday)\n",
    "    job_details.append(job_detail)\n",
    "    deadlines.append(deadline)\n",
    "    a =({ \n",
    "        '岗位名称':job_names,\n",
    "        '公司名称':comps,\n",
    "        '工资':salaries,\n",
    "        '工作地点':locates,\n",
    "        '学位要求':degrees,\n",
    "        '工作日':weekdays,\n",
    "        '截止日期':deadlines, \n",
    "        '岗位描述':job_details})\n",
    "    df = pd.DataFrame.from_dict(a, orient='index')\n",
    "    df.to_csv(r\"./shixiseng_all4.csv\",sep=',')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\xee\\xb4\\xab'\n"
     ]
    }
   ],
   "source": [
    "#Crack the encrypted numbers one by one\n",
    "print(''.encode('utf-8'))\n",
    "# 150-200/天\n",
    "# '\\xef\\xa3\\x8c\\xee\\xbd\\xac\\xee\\x96\\xb6-\\xee\\xbe\\xaf\\xee\\x96\\xb6\\xee\\x96\\xb6/\\xe5\\xa4\\xa9'\n",
    "# 1 is represented by  \n",
    "# 5 is represented by  \n",
    "# 0 is represented by  \n",
    " \n",
    "<a data-v-98c756d6=\"\" href=\"https://www.shixiseng.com/intern/inn_5azgtucs9yyy?pcm=pc_SearchList\" title=\"阿迪达斯深圳&amp;#xf347号仓实习&amp;#xe5d3\" target=\"_blank\" class=\"title ellipsis font\">阿迪达斯深圳号仓实习</a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<a data-v-98c756d6=\"\" href=\"https://www.shixiseng.com/intern/inn_5azgtucs9yyy?pcm=pc_SearchList\" title=\"阿迪达斯深圳&amp;#xf347号仓实习&amp;#xe5d3\" target=\"_blank\" class=\"title ellipsis font\">阿迪达斯深圳号仓实习</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'jieba'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-74626676bd0a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mjieba\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyecharts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcharts\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'jieba'"
     ]
    }
   ],
   "source": [
    "#Word frequency analysis\n",
    "pip install jieba\n",
    "import pandas as  pd\n",
    "import re\n",
    "import collections\n",
    "import jieba\n",
    "\n",
    "from pyecharts.charts import WordCloud\n",
    "from pyecharts import options as opts\n",
    "\n",
    "# 1.pre-treated\n",
    "data = pd.read_csv('ZLZP.csv')\n",
    "data_cy = data.copy()\n",
    "A = data_cy['职位信息'].str.strip()\n",
    "data_cy['职位信息'] = A\n",
    "string_data = ''\n",
    "for i in data_cy['职位信息']:\n",
    "    string_data += str(i)\n",
    "\n",
    "# 2.Text preprocessing\n",
    "pattern = re.compile(u'\\t|\\n| |；|\\.|。|：|：\\.|-|:|\\d|;|、|，|\\)|\\(|\\?|\"')\n",
    "string_data = re.sub(pattern, '', string_data)\n",
    "\n",
    "# 3.Text segmentation\n",
    "seg_list_exact = jieba.cut(string_data, cut_all=False)\n",
    "\n",
    "# 4.Filter\n",
    "object_list = []\n",
    "with open('./remove_words.txt', 'r', encoding=\"utf-8\") as fp:\n",
    "    remove_words = fp.read().split()\n",
    "\n",
    "for word in seg_list_exact:\n",
    "    if word not in remove_words and word != ' ' and word != '\\xa0':\n",
    "        object_list.append(word)  \n",
    "\n",
    "# 5.Word frequency count\n",
    "word_counts = collections.Counter(object_list)  \n",
    "word_counts_top200 = word_counts.most_common(200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts_top200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
